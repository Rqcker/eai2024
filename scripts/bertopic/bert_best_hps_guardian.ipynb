{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import optuna\n",
    "import spacy\n",
    "from spacy_cleaner import processing, Cleaner\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from umap import UMAP\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents_by_words(documents, max_words=512):\n",
    "    \"\"\"\n",
    "    Split documents if one document's word count is over than max_words.\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of documents as strings.\n",
    "        max_words (int): Maximum number of words for each document.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of split documents.\n",
    "    \"\"\"\n",
    "    split_documents = []\n",
    "    for doc in documents:\n",
    "        words = doc.split()\n",
    "        num_words = len(words)\n",
    "        if num_words <= max_words:\n",
    "            split_documents.append(doc)\n",
    "        else:\n",
    "            # Split document into segments of max_words\n",
    "            num_segments = num_words // max_words\n",
    "            for i in range(num_segments + 1):\n",
    "                start_idx = i * max_words\n",
    "                end_idx = (i + 1) * max_words\n",
    "                if ' '.join(words[start_idx:end_idx]) != '' or ' '.join(words[start_idx:end_idx]) != ' ':\n",
    "                    split_documents.append(' '.join(words[start_idx:end_idx]))\n",
    "    return split_documents \n",
    " \n",
    "\n",
    "df = pd.read_json('/home/yy2046/Workspace/DCEE2023/datasets/theguardian/all_keywords_data/guardian_all_data', lines=True)\n",
    "df.drop_duplicates(subset=['title'], inplace=True)\n",
    "\n",
    "data = [row.title + ' ' + str(row.content['body']) for index, row in df.iterrows()]\n",
    "\n",
    "''' preprocess '''\n",
    "cleaned_data = []\n",
    "model = spacy.load(\"en_core_web_sm\")\n",
    "cleaner = Cleaner( \n",
    "    model,\n",
    "    processing.remove_stopword_token,\n",
    "    processing.remove_punctuation_token,\n",
    "    processing.remove_email_token,\n",
    "    processing.remove_url_token,\n",
    "    processing.mutate_lemma_token,\n",
    "    \n",
    ")\n",
    "\n",
    "for html_text in data:\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    soup_text = soup.get_text().lower()\n",
    "    cleaned_data.append(soup_text)\n",
    "# print(cleaned_data[0])\n",
    "print('spaCy preprocess start!')\n",
    "cleaned_data = cleaner.clean(cleaned_data)\n",
    "# print(cleaned_data[0])\n",
    "print('spaCy preprocess done!')\n",
    " \n",
    "input_data = split_documents_by_words(cleaned_data, max_words=512)\n",
    "\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "print('Model supported the max length of a document: ', embedding_model.max_seq_length)\n",
    "umap_model = UMAP(n_neighbors=10, n_components=15, random_state=42)\n",
    "cluster_model = KMeans(n_clusters=20, random_state=42)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 1))\n",
    "representation_model = KeyBERTInspired(top_n_words=10, random_state=42)\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    top_n_words=10,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=cluster_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    representation_model=representation_model,\n",
    "\n",
    ")\n",
    "topics, probs = topic_model.fit_transform(input_data)\n",
    "print(topic_model.get_topic_info())\n",
    "# print(topic_model.get_topic_info())\n",
    "topic_model.get_topic_info().to_csv('topic_modelling_topic_info.csv')\n",
    "topic_model.get_document_info(input_data).to_csv(\n",
    "    'topic_modelling_docs_info.csv')\n",
    "for i in range(20):\n",
    "    word_list = []\n",
    "    prob_list = []\n",
    "    topic = topic_model.get_topic(i)\n",
    "    for j in topic:\n",
    "        word_list.append(j[0])\n",
    "        prob_list.append(j[1])\n",
    "    pd.DataFrame({\n",
    "        'word': word_list,\n",
    "        'prob': prob_list\n",
    "    }).to_excel('topic_bert' + str(i) + '.xlsx')\n",
    "    print(topic_model.get_topic(i)) \n",
    "'''coherence computation'''\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "words = vectorizer.get_feature_names()\n",
    "tokens = [analyzer(doc) for doc in cleaned_data]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "            for topic in range(len(set(topics))-1)]\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                texts=tokens, \n",
    "                                corpus=corpus,\n",
    "                                dictionary=dictionary, \n",
    "                                coherence='c_npmi')\n",
    "coherence = coherence_model.get_coherence()\n",
    "\n",
    "print(coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['australia', 'australian', 'nsw', 'election', 'labor', 'bst', 'senator', 'coalition', 'need', 'leader']\n",
      "['renewable', 'carbon', 'energy', 'emission', 'international', 'environment', 'world', 'resource', 'goal', 'climate']\n",
      "['energy', 'renewable', 'uk', 'gas', 'fuel', 'carbon', 'solar', 'britain', 'emission', 'electricity']\n",
      "['mp', 'corbyn', 'eu', 'britain', 'tory', 'uk', 'brexit', 'labour', 'conservative', 'british']\n",
      "['recycling', 'recycle', 'recyclable', 'plastic', 'waste', 'reuse', 'circular', 'packaging', 'environment', 'environmental']\n",
      "['sustainable', 'environment', 'farming', 'green', 'land', 'grow', 'plan', 'uk', 'farmer', 'produce']\n",
      "['sustainability', 'sustainable', 'environment', 'approach', 'resource', 'ethical', 'consumer', 'environmental', 'growth', 'future']\n",
      "['australia', 'energy', 'renewable', 'nsw', 'australian', '2050', 'emission', 'gas', 'fuel', 'carbon']\n",
      "['candidate', 'biden', 'trump', 'election', 'voter', 'republicans', 'official', 'democrat', 'campaign', 'senator']\n",
      "['vaccine', 'lockdown', 'covid', 'vaccination', 'outbreak', 'vaccinate', 'quarantine', 'coronavirus', 'gmt', 'health']\n",
      "['ftse', 'eurozone', 'gmt', 'trading', 'inflation', 'market', 'uk', 'price', 'investor', 'european']\n",
      "['feel', 'moment', 'want', 'people', 'audience', 'picture', 'say', 'tell', 'die', 'like']\n",
      "['airbnb', 'rent', 'sharing', 'share', 'create', 'offer', 'site', 'host', 'idea', 'uber']\n",
      "['player', 'sport', 'goal', 'liverpool', 'league', 'kick', 'chelsea', 'want', 'arsenal', 'club']\n",
      "['ev', 'tesla', 'car', 'electric', 'charge', 'nissan', 'vehicle', 'emission', 'petrol', 'driver']\n",
      "['epa', 'pfas', 'chemical', 'toxic', 'federal', 'environmental', 'lead', 'pesticide', 'emission', 'official']\n",
      "['ukraine', 'russia', 'ukrainian', 'putin', 'kremlin', 'eu', 'world', 'russian', 'nato', 'kyiv']\n",
      "['samsung', 'smartphone', 'tablet', 'android', 'device', 'price', 'app', 'screen', 'ipad', 'fold']\n",
      "['gambling', 'gamble', 'betting', 'gambler', 'bet', 'casino', 'bookie', 'punter', 'bookmaker', 'gaming']\n",
      "['pga', 'golfer', 'golf', 'fairway', 'tournament', 'mickelson', 'leaderboard', 'spieth', 'championship', 'shoot']\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range (20):\n",
    "    path = '/home/yy2046/Workspace/DCEE2023/results/bert_selected_hps_guardian_res/topic_bert'+ str(i)+'.xlsx'\n",
    "    df = pd.read_excel(path)\n",
    "    print(df['word'].tolist())\n",
    "    results.append(df['word'].tolist())\n",
    "    pd.DataFrame(results).to_csv('/home/yy2046/Workspace/DCEE2023/results/bert_selected_hps_guardian_res/111.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
