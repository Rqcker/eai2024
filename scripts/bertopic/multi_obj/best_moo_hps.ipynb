{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Starting preprocessing...\n",
      "spaCy preprocess start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Progress: 100%|██████████| 708/708 [00:02<00:00, 279.46it/s]\n",
      "[I 2024-09-04 15:49:14,104] A new study created in memory with name: no-name-7b8771c1-e523-478b-b9cb-c66568e9e7ee\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy preprocess done!\n",
      "Document splitting complete.\n",
      "Starting hyperparameter optimisation...\n",
      "Starting trial with n_gram=1, n_clusters=14, n_components=13, n_neighbors=15\n",
      "Model supported the max length of a document:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-04 15:49:21,184] Trial 0 finished with values: [-0.2626720706778148, 0.855847886967033, 8.182748959518293] and parameters: {}. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from optuna.samplers import TPESampler\n",
    "from spacy_cleaner import processing, Cleaner\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import InvertedRBO\n",
    "from umap import UMAP\n",
    "\n",
    "# Set the TOKENIZERS_PARALLELISM environment variable to avoid warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Function to split documents into chunks with a maximum word count\n",
    "def split_documents_by_words(documents, max_words=512):\n",
    "    split_documents = []\n",
    "    for doc in documents:\n",
    "        words = doc.split()\n",
    "        num_words = len(words)\n",
    "        if num_words <= max_words:\n",
    "            split_documents.append(doc)\n",
    "        else:\n",
    "            num_segments = num_words // max_words\n",
    "            for i in range(num_segments + 1):\n",
    "                start_idx = i * max_words\n",
    "                end_idx = (i + 1) * max_words\n",
    "                segment = ' '.join(words[start_idx:end_idx])\n",
    "                if segment.strip():\n",
    "                    split_documents.append(segment)\n",
    "    return split_documents \n",
    "\n",
    "# Function to calculate topic coherence using Gensim's CoherenceModel\n",
    "def calculate_coherence(topic_model, topics, documents):\n",
    "    vectoriser = topic_model.vectorizer_model\n",
    "    analyser = vectoriser.build_analyzer()\n",
    "    tokens = [analyser(doc) for doc in documents]\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "    topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "                   for topic in range(len(set(topics))-1)]\n",
    "    \n",
    "    coherence_model = CoherenceModel(\n",
    "        topics=topic_words, \n",
    "        texts=tokens, \n",
    "        corpus=corpus,\n",
    "        dictionary=dictionary, \n",
    "        coherence='c_npmi'\n",
    "    )\n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "# Function to calculate diversity using InvertedRBO\n",
    "def calculate_diversity(topic_model):\n",
    "    topics = topic_model.get_topics()\n",
    "    topic_words = [[word for word, _ in words] for _, words in topics.items()]\n",
    "    model_output = {\"topics\": topic_words}\n",
    "    diversity_model = InvertedRBO()\n",
    "    return diversity_model.score(model_output)\n",
    "\n",
    "# Function to calculate perplexity from probabilities\n",
    "def calculate_perplexity(probs):\n",
    "    if probs is None or probs.size == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    probs = np.clip(probs, 1e-10, None)\n",
    "    log_perplexity = -1 * np.mean(np.log(np.sum(probs, axis=1)))\n",
    "    return np.exp(log_perplexity)\n",
    "\n",
    "# Objective function for Optuna optimization\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter optimization of BERTopic model.\n",
    "    \"\"\"\n",
    "    # Fixed hyperparameters\n",
    "    n_gram = 1\n",
    "    n_clusters = 14\n",
    "    n_components = 13\n",
    "    n_neighbors = 15\n",
    "    \n",
    "    embedding_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "    print(f'Starting trial with n_gram={n_gram}, n_clusters={n_clusters}, n_components={n_components}, n_neighbors={n_neighbors}')\n",
    "    print('Model supported the max length of a document: ', embedding_model.max_seq_length)\n",
    "    \n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, n_components=n_components, random_state=42)\n",
    "    cluster_model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, n_gram))\n",
    "    representation_model = KeyBERTInspired(top_n_words=10, random_state=42)\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        top_n_words=10,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=cluster_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        representation_model=representation_model,\n",
    "        calculate_probabilities=True\n",
    "    )\n",
    "    \n",
    "    # Fit model and calculate scores\n",
    "    topics, _ = topic_model.fit_transform(input_data)\n",
    "    probs, _ = topic_model.approximate_distribution(input_data)\n",
    "\n",
    "    coherence = calculate_coherence(topic_model, topics, input_data)\n",
    "    diversity = calculate_diversity(topic_model)\n",
    "    perplexity = calculate_perplexity(probs)\n",
    "    \n",
    "    # Save the topic information to a CSV file\n",
    "    topic_model.get_topic_info().to_csv(\"bertopic_moo_topics_reddits.csv\", index=False)\n",
    "    \n",
    "    return coherence, diversity, perplexity\n",
    "\n",
    "# Load and preprocess data\n",
    "print('Loading data...')\n",
    "df = pd.read_json('/datasets/reddit/dcee_reddit.json')\n",
    "df.drop_duplicates(subset=['title', 'selftext'], inplace=True)\n",
    "data = [row.title + ' ' + str(row.selftext) for index, row in df.iterrows()]\n",
    "\n",
    "print('Starting preprocessing...')\n",
    "cleaned_data = []\n",
    "model = spacy.load(\"en_core_web_sm\")\n",
    "cleaner = Cleaner(\n",
    "    model,\n",
    "    processing.remove_stopword_token,\n",
    "    processing.remove_punctuation_token,\n",
    "    processing.remove_email_token,\n",
    "    processing.remove_url_token,\n",
    "    processing.mutate_lemma_token,\n",
    ")\n",
    "\n",
    "# Clean and preprocess text\n",
    "for html_text in data:\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    soup_text = soup.get_text().lower()\n",
    "    cleaned_data.append(soup_text)\n",
    "\n",
    "print('spaCy preprocess start!')\n",
    "cleaned_data = cleaner.clean(cleaned_data)\n",
    "print('spaCy preprocess done!')\n",
    "\n",
    "# Split documents into chunks of max 512 words\n",
    "input_data = split_documents_by_words(cleaned_data, max_words=512)\n",
    "print('Document splitting complete.')\n",
    "\n",
    "# Start the hyperparameter optimization\n",
    "print('Starting hyperparameter optimisation...')\n",
    "study = optuna.create_study(sampler=TPESampler(), directions=[\"maximize\", \"maximize\", \"minimize\"])\n",
    "study.optimize(objective, n_trials=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "spaCy preprocess start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Progress: 100%|██████████| 3921/3921 [00:06<00:00, 653.46it/s]\n",
      "[I 2024-09-04 15:52:10,651] A new study created in memory with name: no-name-1a665a26-5fb0-44f0-ad49-a15476364159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cleaned documents: 3921\n",
      "Document splitting complete.\n",
      "Starting hyperparameter optimisation...\n",
      "Starting trial with n_gram=1, n_clusters=8, n_components=7, n_neighbors=15\n",
      "Model supported the max length of a document:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-04 15:52:36,672] Trial 0 finished with values: [-0.011112568487815349, 0.9486033017372449, 74.92023928231484] and parameters: {}. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from optuna.samplers import TPESampler\n",
    "from spacy_cleaner import processing, Cleaner\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import InvertedRBO\n",
    "from umap import UMAP\n",
    "\n",
    "# Set the TOKENIZERS_PARALLELISM environment variable to avoid warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Function to split documents into chunks with a maximum word count\n",
    "def split_documents_by_words(documents, max_words=512):\n",
    "    split_documents = []\n",
    "    for doc in documents:\n",
    "        words = doc.split()\n",
    "        num_words = len(words)\n",
    "        if num_words <= max_words:\n",
    "            split_documents.append(doc)\n",
    "        else:\n",
    "            for i in range(0, num_words, max_words):\n",
    "                segment = ' '.join(words[i:i + max_words])\n",
    "                if segment.strip():\n",
    "                    split_documents.append(segment)\n",
    "    return split_documents \n",
    "\n",
    "# Function to calculate topic coherence using Gensim's CoherenceModel\n",
    "def calculate_coherence(topic_model, topics, documents):\n",
    "    vectoriser = topic_model.vectorizer_model\n",
    "    analyser = vectoriser.build_analyzer()\n",
    "    tokens = [analyser(doc) for doc in documents]\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "    topic_words = [[words for words, _ in topic_model.get_topic(topic)] for topic in range(len(set(topics))-1)]\n",
    "    \n",
    "    coherence_model = CoherenceModel(\n",
    "        topics=topic_words, \n",
    "        texts=tokens, \n",
    "        corpus=corpus,\n",
    "        dictionary=dictionary, \n",
    "        coherence='c_npmi'\n",
    "    )\n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "# Function to calculate diversity using InvertedRBO\n",
    "def calculate_diversity(topic_model):\n",
    "    topics = topic_model.get_topics()\n",
    "    topic_words = [[word for word, _ in words] for _, words in topics.items()]\n",
    "    model_output = {\"topics\": topic_words}\n",
    "    diversity_model = InvertedRBO()\n",
    "    return diversity_model.score(model_output)\n",
    "\n",
    "# Function to calculate perplexity from probabilities\n",
    "def calculate_perplexity(probs):\n",
    "    if probs is None or probs.size == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    probs = np.clip(probs, 1e-10, None)\n",
    "    log_perplexity = -1 * np.mean(np.log(np.sum(probs, axis=1)))\n",
    "    return np.exp(log_perplexity)\n",
    "\n",
    "# Objective function for Optuna optimization\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter optimization of BERTopic model.\n",
    "    \"\"\"\n",
    "    n_gram = 1\n",
    "    n_clusters = 8\n",
    "    n_components = 7\n",
    "    n_neighbors = 15\n",
    "    \n",
    "    embedding_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "    print(f'Starting trial with n_gram={n_gram}, n_clusters={n_clusters}, n_components={n_components}, n_neighbors={n_neighbors}')\n",
    "    print('Model supports the max length of a document: ', embedding_model.max_seq_length)\n",
    "    \n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, n_components=n_components, random_state=42)\n",
    "    cluster_model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, n_gram))\n",
    "    representation_model = KeyBERTInspired(top_n_words=10, random_state=42)\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        top_n_words=10,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=cluster_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        representation_model=representation_model,\n",
    "    )\n",
    "    \n",
    "    topics, _ = topic_model.fit_transform(input_data)\n",
    "    probs, _ = topic_model.approximate_distribution(input_data)\n",
    "    \n",
    "    coherence = calculate_coherence(topic_model, topics, input_data)\n",
    "    diversity = calculate_diversity(topic_model)\n",
    "    perplexity = calculate_perplexity(probs)\n",
    "    \n",
    "    topic_model.get_topic_info().to_csv(\"bertopic_moo_topics_twitter.csv\", index=False)\n",
    "    return coherence, diversity, perplexity\n",
    "\n",
    "# Load and preprocess data\n",
    "print('Loading data...')\n",
    "df = pd.read_csv('/datasets/twitter/cleaned_tweets.csv', encoding='unicode_escape')\n",
    "data = df['full_text']\n",
    "\n",
    "# Preprocess\n",
    "print('Starting preprocessing...')\n",
    "cleaned_data = []\n",
    "model = spacy.load(\"en_core_web_sm\")\n",
    "cleaner = Cleaner(\n",
    "    model,\n",
    "    processing.remove_stopword_token,\n",
    "    processing.remove_punctuation_token,\n",
    "    processing.remove_email_token,\n",
    "    processing.remove_url_token,\n",
    "    processing.mutate_lemma_token,\n",
    ")\n",
    "\n",
    "for html_text in data:\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    soup_text = soup.get_text().lower()\n",
    "    cleaned_data.append(soup_text)\n",
    "\n",
    "print('spaCy preprocess start!')\n",
    "cleaned_data = cleaner.clean(cleaned_data)\n",
    "print(f'Number of cleaned documents: {len(cleaned_data)}')\n",
    "\n",
    "# Split documents into chunks\n",
    "input_data = split_documents_by_words(cleaned_data, max_words=512)\n",
    "print('Document splitting complete.')\n",
    "\n",
    "# Start the hyperparameter optimization\n",
    "print('Starting hyperparameter optimisation...')\n",
    "study = optuna.create_study(sampler=TPESampler(), directions=[\"maximize\", \"maximize\", \"minimize\"])\n",
    "study.optimize(objective, n_trials=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Progress: 100%|██████████| 17477/17477 [37:31<00:00,  7.76it/s]  \n",
      "[I 2024-09-04 16:34:40,987] A new study created in memory with name: no-name-180a9c0c-7ac5-458d-af61-9ed7ed38019a\n",
      "[I 2024-09-04 16:41:56,737] Trial 0 finished with values: [0.13809031691180196, 0.9886139268051504, 1.4469154722396265] and parameters: {}. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from spacy_cleaner import processing, Cleaner\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.evaluation_metrics.diversity_metrics import InvertedRBO\n",
    "from umap import UMAP\n",
    "\n",
    "# Set environment variable to avoid parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Function to split documents into chunks with a maximum word count\n",
    "def split_documents_by_words(documents, max_words=512):\n",
    "    split_documents = []\n",
    "    for doc in documents:\n",
    "        words = doc.split()\n",
    "        if len(words) <= max_words:\n",
    "            split_documents.append(doc)\n",
    "        else:\n",
    "            for i in range(0, len(words), max_words):\n",
    "                segment = ' '.join(words[i:i + max_words])\n",
    "                if segment.strip():\n",
    "                    split_documents.append(segment)\n",
    "    return split_documents\n",
    "\n",
    "# Function to calculate topic coherence using c_npmi metric\n",
    "def calculate_coherence(topic_model, documents):\n",
    "    topics = topic_model.get_topics()\n",
    "    topic_words = [[word for word, _ in words] for _, words in topics.items()]\n",
    "    \n",
    "    coherence_model = Coherence(\n",
    "        texts=[doc.split() for doc in documents], \n",
    "        topk=10, \n",
    "        measure='c_npmi'\n",
    "    )\n",
    "    model_output = {\"topics\": topic_words}\n",
    "    coherence_score = coherence_model.score(model_output)\n",
    "    \n",
    "    return coherence_score\n",
    "\n",
    "# Function to calculate topic diversity using InvertedRBO metric\n",
    "def calculate_diversity(topic_model):\n",
    "    topics = topic_model.get_topics()\n",
    "    topic_words = [[word for word, _ in words] for _, words in topics.items()]\n",
    "    \n",
    "    model_output = {\"topics\": topic_words}\n",
    "    diversity_model = InvertedRBO()\n",
    "    diversity_score = diversity_model.score(model_output)\n",
    "    \n",
    "    return diversity_score\n",
    "\n",
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(probs):\n",
    "    if probs is None or probs.size == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    probs = np.clip(probs, 1e-10, None)  # Avoid division by zero\n",
    "    log_perplexity = -1 * np.mean(np.log(np.sum(probs, axis=1)))\n",
    "    \n",
    "    return np.exp(log_perplexity)\n",
    "\n",
    "# Objective function for Optuna optimization\n",
    "def objective(trial):\n",
    "    n_gram = 1\n",
    "    n_clusters = 20\n",
    "    n_components = 11\n",
    "    n_neighbors = 19\n",
    "    \n",
    "    embedding_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, n_components=n_components, random_state=42)\n",
    "    cluster_model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, n_gram))\n",
    "    representation_model = KeyBERTInspired(top_n_words=10, random_state=42)\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        top_n_words=10,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=cluster_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        representation_model=representation_model,\n",
    "        calculate_probabilities=True\n",
    "    )\n",
    "    \n",
    "    topic_model.fit_transform(input_data)\n",
    "    probs, _ = topic_model.approximate_distribution(input_data)\n",
    "    \n",
    "    coherence = calculate_coherence(topic_model, input_data)\n",
    "    diversity = calculate_diversity(topic_model)\n",
    "    perplexity = calculate_perplexity(probs)\n",
    "    \n",
    "    topic_model.get_topic_info().to_csv(\"bertopic_moo_topics_guardian.csv\", index=False)\n",
    "    \n",
    "    return coherence, diversity, perplexity\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_json('/datasets/theguardian/dcee_guardian', lines=True)\n",
    "df.drop_duplicates(subset=['title'], inplace=True)\n",
    "data = [f\"{row.title} {str(row.content['body'])}\" for _, row in df.iterrows()]\n",
    "\n",
    "# Text cleaning and preprocessing\n",
    "cleaned_data = []\n",
    "model = spacy.load(\"en_core_web_sm\")\n",
    "cleaner = Cleaner(\n",
    "    model,\n",
    "    processing.remove_stopword_token,\n",
    "    processing.remove_punctuation_token,\n",
    "    processing.remove_email_token,\n",
    "    processing.remove_url_token,\n",
    "    processing.mutate_lemma_token,\n",
    ")\n",
    "\n",
    "for html_text in data:\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    soup_text = soup.get_text().lower()\n",
    "    cleaned_data.append(soup_text)\n",
    "\n",
    "cleaned_data = cleaner.clean(cleaned_data)\n",
    "input_data = split_documents_by_words(cleaned_data, max_words=512)\n",
    "\n",
    "# Hyperparameter optimization using Optuna\n",
    "study = optuna.create_study(\n",
    "    directions=[\"maximize\", \"maximize\", \"minimize\"], \n",
    "    sampler=optuna.samplers.TPESampler()\n",
    ")\n",
    "study.optimize(objective, n_trials=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
